{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e2ff52-d93e-4202-8330-b473533ee6f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 现在我们基本了解了bedrock的sdk, knowledge base和langchain集成的接口。现在我们进行一个实践项目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5527c",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d722e7f",
   "metadata": {},
   "source": [
    "如今很多交易所要求上市公司出了财报以外，还需提交ESG报告，披露公司在环境、社会和公司治理等方面信息，ESG报告的撰写是高度专业性工作，内容广泛，要求复杂。而生成式AI的发展，很大程度上可以简化ESG报告的撰写。利用 RAG 技术，结合企业私有ESG等数据，基于 LLM 和智能搜索，以低代码/无代码的形式，简单快速地构建企业私域知识库，配合提示词工程，可为企业提供高质量的写作建议，甚至生成报告。接下来我们让我们一起体验Bedrock Knowledge Base和Claude3如何帮助公司生成ESG报告。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499dddc",
   "metadata": {},
   "source": [
    "## Bedrock Knowledge Base 检索ESG报告相关内容并生成回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d7d41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import botocore.session\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "question = 'ZTO Express Water Management'\n",
    "\n",
    "# 1. set up bedrock knowledge base client and retrieve\n",
    "kb_client = boto3.client('bedrock-agent-runtime')\n",
    "kb_response = kb_client.retrieve(\n",
    "    knowledgeBaseId='your-bedrock-konwledgebase-id',\n",
    "    retrievalQuery={\n",
    "        'text': question\n",
    "    },\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 10,\n",
    "            'overrideSearchType': 'HYBRID'\n",
    "        }\n",
    "    }\n",
    "    #nextToken='string'\n",
    ")\n",
    "context = \"\"\n",
    "for result in kb_response['retrievalResults']:\n",
    "    context = context +'\\n'+ result['content']['text']\n",
    "    print(result['location'])\n",
    "    print(result['content']['text'])\n",
    "    print(\"----------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363d46a",
   "metadata": {},
   "source": [
    "## Claude3 生成报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb55770",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. set up llm - claude3\n",
    "claude3_kwargs = {\n",
    "    \"temperature\":0.0001,\n",
    "    #\"temperature\":1,\n",
    "    \"top_k\":250,\n",
    "    \"top_p\":0.999,\n",
    "    \"max_tokens\": 3000,\n",
    "}\n",
    "claude3 = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs=claude3_kwargs\n",
    ")\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_01 = LLMChain(llm=claude3, prompt=prompt)\n",
    "\n",
    "response_01=chain_01.invoke({'question': question,'context': context})\n",
    "print(response_01['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
